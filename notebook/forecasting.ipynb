{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample for pytorch forecasting\n",
    "- c.f. https://towardsdatascience.com/introducing-pytorch-forecasting-64de99b9ef46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.8/dist-packages/fastparquet/parquet_thrift/parquet/ttypes.py:1929: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n  iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])\n/usr/local/lib/python3.8/dist-packages/fastparquet/parquet_thrift/parquet/ttypes.py:975: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n  iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])\n/usr/local/lib/python3.8/dist-packages/fastparquet/parquet_thrift/parquet/ttypes.py:975: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n  iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])\n"
    }
   ],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "data = get_stallion_data()  # load data as pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          agency     sku     volume       date  industry_volume  soda_volume  \\\n291    Agency_25  SKU_03     0.5076 2013-01-01        492612703    718394219   \n871    Agency_29  SKU_02     8.7480 2015-01-01        498567142    762225057   \n19532  Agency_47  SKU_01     4.9680 2013-09-01        454252482    789624076   \n2089   Agency_53  SKU_07    21.6825 2013-10-01        480693900    791658684   \n9755   Agency_17  SKU_02   960.5520 2015-03-01        515468092    871204688   \n7561   Agency_05  SKU_03  1184.6535 2014-02-01        425528909    734443953   \n19204  Agency_11  SKU_05     5.5593 2017-08-01        623319783   1049868815   \n8781   Agency_48  SKU_04  4275.1605 2013-03-01        509281531    892192092   \n2540   Agency_07  SKU_21     0.0000 2015-10-01        544203593    761469815   \n12084  Agency_21  SKU_03    46.3608 2017-04-01        589969396    940912941   \n\n       avg_max_temp  price_regular  price_actual    discount  ...  \\\n291       25.845238    1264.162234   1152.473405  111.688829  ...   \n871       27.584615    1316.098485   1296.804924   19.293561  ...   \n19532     30.665957    1269.250000   1266.490490    2.759510  ...   \n2089      29.197727    1193.842373   1128.124395   65.717978  ...   \n9755      23.608120    1338.334248   1232.128069  106.206179  ...   \n7561      28.668254    1369.556376   1161.135214  208.421162  ...   \n19204     31.915385    1922.486644   1651.307674  271.178970  ...   \n8781      26.767857    1761.258209   1546.059670  215.198539  ...   \n2540      28.987755       0.000000      0.000000    0.000000  ...   \n12084     32.478910    1675.922116   1413.571789  262.350327  ...   \n\n       football_gold_cup  beer_capital  music_fest discount_in_percent  \\\n291                    -             -           -            8.835008   \n871                    -             -           -            1.465966   \n19532                  -             -           -            0.217413   \n2089                   -  beer_capital           -            5.504745   \n9755                   -             -  music_fest            7.935699   \n7561                   -             -           -           15.218151   \n19204                  -             -           -           14.105636   \n8781                   -             -  music_fest           12.218455   \n2540                   -             -           -            0.000000   \n12084                  -             -           -           15.654088   \n\n      timeseries time_idx month log_volume avg_volume_by_sku  \\\n291          228        0     1  -0.678062       1225.306376   \n871          177       24     1   2.168825       1634.434615   \n19532        322        8     9   1.603017       2625.472644   \n2089         240        9    10   3.076505         38.529107   \n9755         259       26     3   6.867508       2143.677462   \n7561          21       13     2   7.077206       1566.643589   \n19204         17       55     8   1.715472       1385.225478   \n8781         151        2     3   8.360577       1757.950603   \n2540         300       33    10 -18.420681          0.000000   \n12084        181       51     4   3.836454       2034.293024   \n\n      avg_volume_by_agency  \n291              99.650400  \n871              11.397086  \n19532            48.295650  \n2089           2511.035175  \n9755            396.022140  \n7561           1881.866367  \n19204           109.699200  \n8781           1925.272108  \n2540           2418.719550  \n12084           109.381800  \n\n[10 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>agency</th>\n      <th>sku</th>\n      <th>volume</th>\n      <th>date</th>\n      <th>industry_volume</th>\n      <th>soda_volume</th>\n      <th>avg_max_temp</th>\n      <th>price_regular</th>\n      <th>price_actual</th>\n      <th>discount</th>\n      <th>...</th>\n      <th>football_gold_cup</th>\n      <th>beer_capital</th>\n      <th>music_fest</th>\n      <th>discount_in_percent</th>\n      <th>timeseries</th>\n      <th>time_idx</th>\n      <th>month</th>\n      <th>log_volume</th>\n      <th>avg_volume_by_sku</th>\n      <th>avg_volume_by_agency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>291</th>\n      <td>Agency_25</td>\n      <td>SKU_03</td>\n      <td>0.5076</td>\n      <td>2013-01-01</td>\n      <td>492612703</td>\n      <td>718394219</td>\n      <td>25.845238</td>\n      <td>1264.162234</td>\n      <td>1152.473405</td>\n      <td>111.688829</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>8.835008</td>\n      <td>228</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.678062</td>\n      <td>1225.306376</td>\n      <td>99.650400</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>Agency_29</td>\n      <td>SKU_02</td>\n      <td>8.7480</td>\n      <td>2015-01-01</td>\n      <td>498567142</td>\n      <td>762225057</td>\n      <td>27.584615</td>\n      <td>1316.098485</td>\n      <td>1296.804924</td>\n      <td>19.293561</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>1.465966</td>\n      <td>177</td>\n      <td>24</td>\n      <td>1</td>\n      <td>2.168825</td>\n      <td>1634.434615</td>\n      <td>11.397086</td>\n    </tr>\n    <tr>\n      <th>19532</th>\n      <td>Agency_47</td>\n      <td>SKU_01</td>\n      <td>4.9680</td>\n      <td>2013-09-01</td>\n      <td>454252482</td>\n      <td>789624076</td>\n      <td>30.665957</td>\n      <td>1269.250000</td>\n      <td>1266.490490</td>\n      <td>2.759510</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>0.217413</td>\n      <td>322</td>\n      <td>8</td>\n      <td>9</td>\n      <td>1.603017</td>\n      <td>2625.472644</td>\n      <td>48.295650</td>\n    </tr>\n    <tr>\n      <th>2089</th>\n      <td>Agency_53</td>\n      <td>SKU_07</td>\n      <td>21.6825</td>\n      <td>2013-10-01</td>\n      <td>480693900</td>\n      <td>791658684</td>\n      <td>29.197727</td>\n      <td>1193.842373</td>\n      <td>1128.124395</td>\n      <td>65.717978</td>\n      <td>...</td>\n      <td>-</td>\n      <td>beer_capital</td>\n      <td>-</td>\n      <td>5.504745</td>\n      <td>240</td>\n      <td>9</td>\n      <td>10</td>\n      <td>3.076505</td>\n      <td>38.529107</td>\n      <td>2511.035175</td>\n    </tr>\n    <tr>\n      <th>9755</th>\n      <td>Agency_17</td>\n      <td>SKU_02</td>\n      <td>960.5520</td>\n      <td>2015-03-01</td>\n      <td>515468092</td>\n      <td>871204688</td>\n      <td>23.608120</td>\n      <td>1338.334248</td>\n      <td>1232.128069</td>\n      <td>106.206179</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>music_fest</td>\n      <td>7.935699</td>\n      <td>259</td>\n      <td>26</td>\n      <td>3</td>\n      <td>6.867508</td>\n      <td>2143.677462</td>\n      <td>396.022140</td>\n    </tr>\n    <tr>\n      <th>7561</th>\n      <td>Agency_05</td>\n      <td>SKU_03</td>\n      <td>1184.6535</td>\n      <td>2014-02-01</td>\n      <td>425528909</td>\n      <td>734443953</td>\n      <td>28.668254</td>\n      <td>1369.556376</td>\n      <td>1161.135214</td>\n      <td>208.421162</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>15.218151</td>\n      <td>21</td>\n      <td>13</td>\n      <td>2</td>\n      <td>7.077206</td>\n      <td>1566.643589</td>\n      <td>1881.866367</td>\n    </tr>\n    <tr>\n      <th>19204</th>\n      <td>Agency_11</td>\n      <td>SKU_05</td>\n      <td>5.5593</td>\n      <td>2017-08-01</td>\n      <td>623319783</td>\n      <td>1049868815</td>\n      <td>31.915385</td>\n      <td>1922.486644</td>\n      <td>1651.307674</td>\n      <td>271.178970</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>14.105636</td>\n      <td>17</td>\n      <td>55</td>\n      <td>8</td>\n      <td>1.715472</td>\n      <td>1385.225478</td>\n      <td>109.699200</td>\n    </tr>\n    <tr>\n      <th>8781</th>\n      <td>Agency_48</td>\n      <td>SKU_04</td>\n      <td>4275.1605</td>\n      <td>2013-03-01</td>\n      <td>509281531</td>\n      <td>892192092</td>\n      <td>26.767857</td>\n      <td>1761.258209</td>\n      <td>1546.059670</td>\n      <td>215.198539</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>music_fest</td>\n      <td>12.218455</td>\n      <td>151</td>\n      <td>2</td>\n      <td>3</td>\n      <td>8.360577</td>\n      <td>1757.950603</td>\n      <td>1925.272108</td>\n    </tr>\n    <tr>\n      <th>2540</th>\n      <td>Agency_07</td>\n      <td>SKU_21</td>\n      <td>0.0000</td>\n      <td>2015-10-01</td>\n      <td>544203593</td>\n      <td>761469815</td>\n      <td>28.987755</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>0.000000</td>\n      <td>300</td>\n      <td>33</td>\n      <td>10</td>\n      <td>-18.420681</td>\n      <td>0.000000</td>\n      <td>2418.719550</td>\n    </tr>\n    <tr>\n      <th>12084</th>\n      <td>Agency_21</td>\n      <td>SKU_03</td>\n      <td>46.3608</td>\n      <td>2017-04-01</td>\n      <td>589969396</td>\n      <td>940912941</td>\n      <td>32.478910</td>\n      <td>1675.922116</td>\n      <td>1413.571789</td>\n      <td>262.350327</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>15.654088</td>\n      <td>181</td>\n      <td>51</td>\n      <td>4</td>\n      <td>3.836454</td>\n      <td>2034.293024</td>\n      <td>109.381800</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "# categories have to be strings\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"sku\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "data[\"avg_volume_by_agency\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"agency\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "# we want to encode special days as one variable and \n",
    "# thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\", \"good_friday\", \"new_year\", \"christmas\",\n",
    "    \"labor_day\", \"independence_day\", \"revolution_day_memorial\",\n",
    "    \"regional_games\", \"fifa_u_17_world_cup\", \"football_gold_cup\",\n",
    "    \"beer_capital\", \"music_fest\"\n",
    "]\n",
    "data[special_days] = (\n",
    "    data[special_days]\n",
    "    .apply(lambda x: x.map({0: \"-\", 1: x.name}))\n",
    "    .astype(\"category\")\n",
    ")\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    }
   ],
   "source": [
    "from pytorch_forecasting.data import (\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer\n",
    ")\n",
    "max_prediction_length = 6  # forecast 6 months\n",
    "max_encoder_length = 24  # use 24 months of history\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=0,  # allow predictions without history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\n",
    "        \"avg_population_2017\",\n",
    "        \"avg_yearly_household_income_2017\"\n",
    "    ],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # group of categorical variables can be treated as \n",
    "    # one variable\n",
    "    variable_groups={\"special_days\": special_days},\n",
    "    time_varying_known_reals=[\n",
    "        \"time_idx\",\n",
    "        \"price_regular\",\n",
    "        \"discount_in_percent\"\n",
    "    ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], coerce_positive=1.0\n",
    "    ),  # use softplus with beta=1.0 and normalize by group\n",
    "    add_relative_time_idx=True,  # add as feature\n",
    "    add_target_scales=True,  # add as feature\n",
    "    add_encoder_length=True,  # add as feature\n",
    ")\n",
    "# create validation set (predict=True) which means to predict the\n",
    "# last max_prediction_length points in time for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n",
    "# create dataloaders for model\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\n\n   | Name                               | Type                            | Params\n----------------------------------------------------------------------------------------\n0  | loss                               | QuantileLoss                    | 0     \n1  | input_embeddings                   | ModuleDict                      | 1 K   \n2  | prescalers                         | ModuleDict                      | 256   \n3  | static_variable_selection          | VariableSelectionNetwork        | 3 K   \n4  | encoder_variable_selection         | VariableSelectionNetwork        | 8 K   \n5  | decoder_variable_selection         | VariableSelectionNetwork        | 2 K   \n6  | static_context_variable_selection  | GatedResidualNetwork            | 1 K   \n7  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1 K   \n8  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1 K   \n9  | static_context_enrichment          | GatedResidualNetwork            | 1 K   \n10 | lstm_encoder                       | LSTM                            | 2 K   \n11 | lstm_decoder                       | LSTM                            | 2 K   \n12 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n13 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n14 | static_enrichment                  | GatedResidualNetwork            | 1 K   \n15 | multihead_attn                     | InterpretableMultiHeadAttention | 1 K   \n16 | post_attn_gate_norm                | GateAddNorm                     | 576   \n17 | pos_wise_ff                        | GatedResidualNetwork            | 1 K   \n18 | pre_output_gate_norm               | GateAddNorm                     | 576   \n19 | output_layer                       | Linear                          | 119   \n29625\nEpoch 0:  97%|█████████▋| 30/31 [00:10<00:00,  2.96it/s, loss=102.684, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 0: 100%|██████████| 31/31 [00:11<00:00,  2.72it/s, loss=102.684, v_num=1]\nEpoch 1:  97%|█████████▋| 30/31 [00:10<00:00,  2.81it/s, loss=73.124, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 1: 100%|██████████| 31/31 [00:11<00:00,  2.61it/s, loss=73.124, v_num=1]\nEpoch 2:  97%|█████████▋| 30/31 [00:10<00:00,  2.87it/s, loss=74.802, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 2: 100%|██████████| 31/31 [00:11<00:00,  2.68it/s, loss=74.802, v_num=1]\nEpoch 3:  97%|█████████▋| 30/31 [00:10<00:00,  2.90it/s, loss=65.841, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 3: 100%|██████████| 31/31 [00:11<00:00,  2.68it/s, loss=65.841, v_num=1]\nEpoch 4:  97%|█████████▋| 30/31 [00:10<00:00,  2.95it/s, loss=66.531, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 4: 100%|██████████| 31/31 [00:11<00:00,  2.76it/s, loss=66.531, v_num=1]\nEpoch 5:  97%|█████████▋| 30/31 [00:10<00:00,  2.92it/s, loss=61.265, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 5: 100%|██████████| 31/31 [00:11<00:00,  2.72it/s, loss=61.265, v_num=1]\nEpoch 6:  97%|█████████▋| 30/31 [00:09<00:00,  3.14it/s, loss=60.024, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 6: 100%|██████████| 31/31 [00:10<00:00,  2.89it/s, loss=60.024, v_num=1]\nEpoch 7:  97%|█████████▋| 30/31 [00:09<00:00,  3.11it/s, loss=61.035, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 7: 100%|██████████| 31/31 [00:10<00:00,  2.87it/s, loss=61.035, v_num=1]\nEpoch 8:  97%|█████████▋| 30/31 [00:10<00:00,  2.95it/s, loss=58.993, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 8: 100%|██████████| 31/31 [00:11<00:00,  2.74it/s, loss=58.993, v_num=1]\nEpoch 9:  97%|█████████▋| 30/31 [00:09<00:00,  3.00it/s, loss=57.824, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 9: 100%|██████████| 31/31 [00:11<00:00,  2.71it/s, loss=57.824, v_num=1]\nEpoch 10:  97%|█████████▋| 30/31 [00:10<00:00,  2.88it/s, loss=60.904, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 10: 100%|██████████| 31/31 [00:11<00:00,  2.69it/s, loss=60.904, v_num=1]\nEpoch 11:  97%|█████████▋| 30/31 [00:10<00:00,  2.89it/s, loss=59.286, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 11: 100%|██████████| 31/31 [00:11<00:00,  2.70it/s, loss=59.286, v_num=1]\nEpoch 12:  97%|█████████▋| 30/31 [00:10<00:00,  2.76it/s, loss=56.583, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 12: 100%|██████████| 31/31 [00:11<00:00,  2.59it/s, loss=56.583, v_num=1]\nEpoch 13:  97%|█████████▋| 30/31 [00:10<00:00,  3.00it/s, loss=54.381, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 13: 100%|██████████| 31/31 [00:11<00:00,  2.77it/s, loss=54.381, v_num=1]\nEpoch 14:  97%|█████████▋| 30/31 [00:10<00:00,  2.88it/s, loss=51.488, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 14: 100%|██████████| 31/31 [00:11<00:00,  2.67it/s, loss=51.488, v_num=1]\nEpoch 15:  97%|█████████▋| 30/31 [00:09<00:00,  3.03it/s, loss=51.292, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 15: 100%|██████████| 31/31 [00:10<00:00,  2.82it/s, loss=51.292, v_num=1]\nEpoch 16:  97%|█████████▋| 30/31 [00:10<00:00,  2.86it/s, loss=48.058, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 16: 100%|██████████| 31/31 [00:11<00:00,  2.65it/s, loss=48.058, v_num=1]\nEpoch 17:  97%|█████████▋| 30/31 [00:09<00:00,  3.08it/s, loss=48.868, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 17: 100%|██████████| 31/31 [00:11<00:00,  2.81it/s, loss=48.868, v_num=1]\nEpoch 18:  97%|█████████▋| 30/31 [00:09<00:00,  3.24it/s, loss=50.840, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 18: 100%|██████████| 31/31 [00:10<00:00,  2.93it/s, loss=50.840, v_num=1]\nEpoch 19:  97%|█████████▋| 30/31 [00:08<00:00,  3.38it/s, loss=48.940, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 19: 100%|██████████| 31/31 [00:10<00:00,  3.06it/s, loss=48.940, v_num=1]\nEpoch 20:  97%|█████████▋| 30/31 [00:09<00:00,  3.27it/s, loss=49.161, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 20: 100%|██████████| 31/31 [00:10<00:00,  2.97it/s, loss=49.161, v_num=1]\nEpoch 21:  97%|█████████▋| 30/31 [00:09<00:00,  3.20it/s, loss=46.725, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 21: 100%|██████████| 31/31 [00:11<00:00,  2.79it/s, loss=46.725, v_num=1]\nEpoch 22:  97%|█████████▋| 30/31 [00:09<00:00,  3.16it/s, loss=48.535, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 22: 100%|██████████| 31/31 [00:10<00:00,  2.91it/s, loss=48.535, v_num=1]\nEpoch 23:  97%|█████████▋| 30/31 [00:09<00:00,  3.25it/s, loss=45.138, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 23: 100%|██████████| 31/31 [00:10<00:00,  2.99it/s, loss=45.138, v_num=1]\nEpoch 24:  97%|█████████▋| 30/31 [00:09<00:00,  3.20it/s, loss=41.516, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 24: 100%|██████████| 31/31 [00:10<00:00,  2.95it/s, loss=41.516, v_num=1]\nEpoch 25:  97%|█████████▋| 30/31 [00:09<00:00,  3.20it/s, loss=43.220, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 25: 100%|██████████| 31/31 [00:10<00:00,  2.94it/s, loss=43.220, v_num=1]\nEpoch 26:  97%|█████████▋| 30/31 [00:09<00:00,  3.21it/s, loss=42.876, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 26: 100%|██████████| 31/31 [00:10<00:00,  2.93it/s, loss=42.876, v_num=1]\nEpoch 27:  97%|█████████▋| 30/31 [00:09<00:00,  3.02it/s, loss=42.478, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 27: 100%|██████████| 31/31 [00:11<00:00,  2.80it/s, loss=42.478, v_num=1]\nEpoch 28:  97%|█████████▋| 30/31 [00:09<00:00,  3.29it/s, loss=41.306, v_num=1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 28: 100%|██████████| 31/31 [00:10<00:00,  3.00it/s, loss=41.306, v_num=1]\n                                                         \u001b[ASaving latest checkpoint..\nEpoch 28: 100%|██████████| 31/31 [00:11<00:00,  2.81it/s, loss=41.306, v_num=1]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateLogger\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "# stop training, when loss metric does not improve on validation set\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateLogger()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"../result/lightning_logs\")  # log to tensorboard\n",
    "# create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,  # train on CPU, use gpus = [0] to run on GPU\n",
    "    # gpus=[1],  # for GPU\n",
    "    gradient_clip_val=0.1,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    limit_train_batches=30,  # running validation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
    "    callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "# initialise model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # biggest influence network size\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # log example every 10 batches\n",
    "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
    ")\n",
    "print(tft.size())   # 29.6k parameters in model\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "best_model_path: ../result/lightning_logs/default/version_1/checkpoints/epoch=28.ckpt\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(262.7276)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "# load the best model according to the validation loss (given that\n",
    "# we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(\"best_model_path:\", best_model_path)\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# calculate mean absolute error on validation set\n",
    "actuals = torch.cat([y for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "MAE()(predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}